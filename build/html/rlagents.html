

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>rlagents module &mdash; Designing Experiments with Neural Networks 1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Designing Experiments with Neural Networks 1 documentation" href="index.html"/>
        <link rel="next" title="environments module" href="environments.html"/>
        <link rel="prev" title="analyzers module" href="analyzers.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Designing Experiments with Neural Networks
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">INtroduccion</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#list">List</a></li>
<li class="toctree-l1"><a class="reference internal" href="analyzers.html">analyzers module</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">rlagents module</a></li>
<li class="toctree-l1"><a class="reference internal" href="environments.html">environments module</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Designing Experiments with Neural Networks</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>rlagents module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rlagents.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-rlagents">
<span id="rlagents-module"></span><h1>rlagents module<a class="headerlink" href="#module-rlagents" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="rlagents.DQN">
<em class="property">class </em><code class="sig-prename descclassname">rlagents.</code><code class="sig-name descname">DQN</code><span class="sig-paren">(</span><em class="sig-param">iterations</em>, <em class="sig-param">input_size</em>, <em class="sig-param">output_size</em>, <em class="sig-param">learning_rate=0.01</em>, <em class="sig-param">discount=0</em>, <em class="sig-param">exploration_rate=1</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Agent based on a simple Deep Q-Network with inputs and outputs. It
uses online learning (no replay memory) to train the train a simple
feedforward network</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iterations</strong> (<em>int</em>) – Number of iterations for the training. This parameter
is used to calculate the decrease on the exploration
rate.</p></li>
<li><p><strong>input_size</strong> (<em>int</em>) – Number of features given to the agent</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Number of actions available for the agent.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate for Q-Learning update rule,
defaults to 0.01</p></li>
<li><p><strong>discount</strong> (<em>int</em><em>, </em><em>optional</em>) – Discount factor. If it’s 0 only cares about
immediate reward. The closer to one the more it values
future rewards, defaults to 0, defaults to 0</p></li>
<li><p><strong>exploration_rate</strong> (<em>int</em><em>, </em><em>optional</em>) – Initial exploration rate for the epsilon-greedy
decision algorithm, defaults to 1, defaults to 1</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="rlagents.DQN.build_model">
<code class="sig-name descname">build_model</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.build_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses keras to build de model. You should modify here the code to
change the neural network architecture. Default to a
<em>input_sizex64x64xoutput_size</em> fully connected sequential network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Keras model for the agent using the specified structure.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.flat">
<code class="sig-name descname">flat</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.flat" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes the state to a flat numpy array</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy array</em>) – Input state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Flat input state</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.get_Q">
<code class="sig-name descname">get_Q</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.get_Q" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Q-Values for all the actions using the trained Deep
Q-Network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy array</em>) – Input state for the agent.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A ordered table with the Q-Value of each action for the given
state.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array? (I think)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.get_next_action">
<code class="sig-name descname">get_next_action</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.get_next_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the model and the exploration rate to choose a an action with an
epsilon-greedy decision algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.greedy_action">
<code class="sig-name descname">greedy_action</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.greedy_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the index of the maximum Q-Value to choose an action.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the model action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.online_train">
<code class="sig-name descname">online_train</code><span class="sig-paren">(</span><em class="sig-param">old_state</em>, <em class="sig-param">old_action</em>, <em class="sig-param">reward</em>, <em class="sig-param">current_state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.online_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses keras built in functions to estimate the values of the
previous state and the future state to calculate the temporal
difference and use it to fit the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>old_state</strong> (<em>numpy array?</em>) – state from which the action was taken.</p></li>
<li><p><strong>old_action</strong> (<em>int</em>) – action that was taken</p></li>
<li><p><strong>reward</strong> (<em>float</em>) – reward obtained from the result of old_action</p></li>
<li><p><strong>current_state</strong> (<em>numpy array?</em>) – state after the action old_action was taken</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.DQN.random_action">
<code class="sig-name descname">random_action</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.DQN.random_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a random integer to choose a random action.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the random action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlagents.Q_Learning">
<em class="property">class </em><code class="sig-prename descclassname">rlagents.</code><code class="sig-name descname">Q_Learning</code><span class="sig-paren">(</span><em class="sig-param">iterations</em>, <em class="sig-param">output_size</em>, <em class="sig-param">learning_rate=0.1</em>, <em class="sig-param">discount=0</em>, <em class="sig-param">exploration_rate=1</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.Q_Learning" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Agent based on the classic Q-Learning algorithm with no inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iterations</strong> (<em>int</em>) – Number of iterations for the training. This parameter
is used to calculate the decrease on the exploration
rate.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Number of actions available for the agent.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate for Q-Learning update rule, defaults
to 0.1</p></li>
<li><p><strong>discount</strong> (<em>int</em><em>, </em><em>optional</em>) – Discount factor. If it’s 0 only cares about
immediate reward. The closer to one the more
it values future rewards, defaults to 0</p></li>
<li><p><strong>exploration_rate</strong> (<em>int</em><em>, </em><em>optional</em>) – Initial exploration rate for the epsilon-greedy
decision algorithm, defaults to 1</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="rlagents.Q_Learning.get_next_action">
<code class="sig-name descname">get_next_action</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.Q_Learning.get_next_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the model and the exploration rate to choose a an action with an
epsilon-greedy decision algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.Q_Learning.greedy_action">
<code class="sig-name descname">greedy_action</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.Q_Learning.greedy_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the index of the maximum Q-Value to choose an action.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the model action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.Q_Learning.random_action">
<code class="sig-name descname">random_action</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.Q_Learning.random_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a random integer to choose a random action.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Integer representing the random action</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlagents.Q_Learning.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">action</em>, <em class="sig-param">reward</em><span class="sig-paren">)</span><a class="headerlink" href="#rlagents.Q_Learning.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the last action taken and the reward obtained to update the
Q-Value of the action by using the Bellman equation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>action</strong> (<em>int</em>) – Last action taken by the agent</p></li>
<li><p><strong>reward</strong> (<em>float</em>) – Reward obtained with the taken action</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="environments.html" class="btn btn-neutral float-right" title="environments module" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="analyzers.html" class="btn btn-neutral" title="analyzers module" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright None so far.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> and ❤️  using a custom <a href="https://github.com/LinxiFan/Stanford-theme">theme</a> based on <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/language_data.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>