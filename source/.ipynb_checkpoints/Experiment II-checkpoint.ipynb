{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\eduar\\anaconda3\\lib\\site-packages (1.16.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b03e9690adb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mN_Episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwo_Sensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mexperimenter1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_Learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_Episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mexperimenter2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoubleDQN_Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_Episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Pruebas sphinx\\source\\environments.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mu, m, g, v, max_reward)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# We calculate the maximum and minimum mass corresponding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;31m#to those limits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Declarations\n",
    "# Imports \n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "from rlagents import Q_Learning, DQN\n",
    "from analyzers import Two_Layers_single_output\n",
    "from environments import Two_Sensors\n",
    "sns.set()\n",
    "import numpy as np\n",
    "\n",
    "N_Episodes = 200\n",
    "count = 0\n",
    "game = Two_Sensors()\n",
    "experimenter1 = Q_Learning(iterations=N_Episodes, output_size=5)\n",
    "experimenter2 = DoubleDQN_Agent(iterations=N_Episodes, input_size=6, output_size=5)\n",
    "analyzer = Analyzer(input_size=5)\n",
    "t0 = time.time()\n",
    "t1 = time.time()\n",
    "\n",
    "# First random initialization of states:\n",
    "action1 = experimenter1.get_next_action()\n",
    "outcome1 = game.take_action(action1)\n",
    "action2 = random.randint(0,4)\n",
    "outcome2 = game.position_exact\n",
    "d_test = 0.001\n",
    "y_predicted = [0]\n",
    "values = []\n",
    "reward = 0\n",
    "marker = 0\n",
    "# Auxiliar variables for collecting data\n",
    "df=pd.DataFrame(columns = ['Zone', 'Exact position', 'Action 1', \n",
    "                           'Outcome 1', 'Action 2', 'Outcome 2', 'Reward', \n",
    "                           'd_predicted', 'd_test', 'Exploration rate'])\n",
    "total_reward_list = []\n",
    "\n",
    "\n",
    "# Main loop\n",
    "while count < N_Episodes:\n",
    "    # Recording data\n",
    "    values.append([game.position_zone, game.position_exact, \n",
    "                   action1, outcome1, action2, outcome2, float(reward), y_predicted[0], d_test, experimenter2.exploration_rate ])\n",
    "    total_reward_list.append(game.total_reward)\n",
    "   \n",
    "    ### Sin esto ha funcionado!! (?Â¿???)\n",
    "    old_state =  np.append(to_categorical(action1, 5), [outcome1])\n",
    "    game.restart_mass()\n",
    "    action1 = experimenter1.get_next_action()\n",
    "    outcome1 = game.take_action(action1)\n",
    "    current_state = np.append(to_categorical(action1, 5), [outcome1])\n",
    "    experimenter2.online_train(old_state, action2, reward, current_state)\n",
    "    action2 = experimenter2.get_next_action(state=current_state)\n",
    "    outcome2 = game.take_action(action2)\n",
    "    \n",
    "    v_train, d_train = game.test_shooting()\n",
    "    measurements = game.get_measurements(action1, outcome1, action2, outcome2, v_train)\n",
    "#     analyzer.remember(d_train, measurements)\n",
    "#     if marker > analyzer.batch:\n",
    "#         marker = 0\n",
    "#         analyzer.train()\n",
    "#         print(count)\n",
    "    \n",
    "    X_train, y_train = game.reshape_for_analyzer(measurements, d_train) \n",
    "    analyzer.train(X_train, y_train)\n",
    "    \n",
    "    #Testing analyzer to generate reward\n",
    "    v_test, d_test = game.test_shooting()\n",
    "    measurements = game.get_measurements(action1,outcome1, action2, outcome2, v_test)\n",
    "    X_test, y_test = game.reshape_for_analyzer(measurements, d_test)\n",
    "    y_predicted = analyzer.predict(X_test)[0]\n",
    "    reward = game.give_reward(y_predicted, d_test)\n",
    "    \n",
    "    # Update experimenter 1\n",
    "    experimenter1.update(action1, reward)\n",
    "\n",
    "# Display training status --------------------------------------\n",
    "    count = count + 1\n",
    "    marker = marker + 1\n",
    "    if count % (N_Episodes/5000) == 0:\n",
    "        clear_output()\n",
    "        t2 = time.time()\n",
    "        m, s = divmod(t2-t1, 60)\n",
    "        mt, st = divmod(t2-t0, 60)\n",
    "        me, se = divmod(((t2-t0)/count)*N_Episodes, 60)\n",
    "        mr, sr = divmod(((t2-t0)/count)*N_Episodes-t2+t0, 60)\n",
    "        print (str(int(count)) + '/' +  str(N_Episodes) + \" episodes\" + '(' + str(100*count/N_Episodes)+ '%)')\n",
    "        print('Elapsed time: {} min {}s'.format(int(mt), int(st)))\n",
    "        print(\"Est. completion time: {} min {}s,  Est. remaining time: {} min {}s\".format(int(me), int(se), int(mr), int(sr)))\n",
    "        t1 = time.time()\n",
    "print('Training completed!')\n",
    "plt.plot(range(len(total_reward_list)), total_reward_list)\n",
    "plt.show()\n",
    "# df=pd.DataFrame(columns = ['Zone', 'Exact position', 'Action 1', \n",
    "#                            'Outcome 1', 'Action 2', 'Outcome 2', 'Reward', \n",
    "#                            'd_predicted', 'd_test', 'Exploration rate'])\n",
    "df=pd.DataFrame(values, columns = ['Zone', 'Exact position', 'Action 1', \n",
    "                                   'Outcome 1', 'Action 2', 'Outcome 2', 'Reward', \n",
    "                                   'd_predicted', 'd_test', 'Exploration rate'])\n",
    "df.to_csv('df_nuevo.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "184cdf6e-8a67-43e9-8238-48f1530b039f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
